% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ellmer.R
\name{perplexica_ask}
\alias{perplexica_ask}
\title{Search first, then ask an LLM (pre-fetch / RAG pattern)}
\usage{
perplexica_ask(
  query,
  chat,
  mode = "speed",
  base_url = perplexica_get_url(),
  verbose = FALSE
)
}
\arguments{
\item{query}{A character string — the user's question.}

\item{chat}{An \code{ellmer} chat object (e.g. from \code{ellmer::chat_openai()}).}

\item{mode}{Optimisation mode passed to \code{\link[=perplexica_search_with_retry]{perplexica_search_with_retry()}}.
\code{"speed"} (default) or \code{"balanced"}.}

\item{base_url}{Base URL of the Perplexica instance.}

\item{verbose}{If \code{TRUE}, prints the query sent to Perplexica. Passed to
\code{\link[=perplexica_search_with_retry]{perplexica_search_with_retry()}}. Default \code{FALSE}.}
}
\value{
The LLM's response string (whatever \code{chat$chat()} returns).
}
\description{
Searches Perplexica for \code{query}, injects the result as grounding context into
the prompt, and calls \code{chat$chat()}. Because the search result is embedded
directly in the message, the LLM is \strong{guaranteed} to reason over live web
information — no tool-calling decision is involved.
}
\details{
Use this instead of \code{\link[=perplexica_tool]{perplexica_tool()}} when every query is a factual
lookup and you need a hard guarantee that Perplexica is always consulted.
For a general-purpose assistant that searches only when needed, register
\code{\link[=perplexica_tool]{perplexica_tool()}} and add a system-prompt directive such as
\code{"Always call perplexica_search before answering. Never rely on prior knowledge alone."}
}
\examples{
\dontrun{
library(ellmer)

chat <- chat_openai(
  model = "gpt-4.1",
  system_prompt = "You are a helpful assistant with access to live web search."
)

perplexica_ask("Who won the most recent FIFA World Cup?", chat = chat)
}
}
